# 从提示词到上下文工程
## 背景
LLMs（大语言模型）的实际效果不只取决于模型参数，推理时提供的 **上下文信息** 也至关重要。
而向 LLMs 提供上下文信息以引导其生成特定类型、主题或输出格式的内容就是**提示词（Prompt）**。
围绕提示词设计、优化、管理及和模型交互策略的整个体系就是**提示词/上下文工程（Prompt/Context Engineering）**。 两者区别在于：前者聚焦于单次交互的静态输入，而后者可应对动态、结构化、多源数据（如：实时数据、知识图谱、历史对话）等复杂情况。可以认为**上下文工程（Context Engineering）** 是对 **提示词工程（Prompt Engineering）** 的升级和加强。
上下文工程（Context Engineering)的目标是：构建一套统一框架，将碎片化技术（如：RAG、记忆系统、多智能体协助等）整合在一起，实现系统化设计、管理和优化 LLMs 信息负载的目的。与提示词工程的主要区别包括：

|维度|提示词工程|上下文工程|
|-------|---------|---------|
|形式|静态字符串|结构化动态组装|
|目标|优化单次提示词|系统级函数优化|
|状态性|无状态|显示记忆与状态管理|
|扩展性|长度增加导致脆弱性|模块化组合管理复杂度|

## 形式化描述
### 组件层
$$C = A(c_1, c_2, ..., c_n)$$
其中 C（**C**ontext / 上下文）是动态结构化信息组件的集合。$c_i$ 代表不同的信息源（例如：指令c<sub>instr</sub>，外部知识c<sub>kb</sub>，记忆c<sub>mem</sub>）。$A$ 是组装函数（例如：优先级排序、模板格式化等），通过引入组装函数解决了灵活整合多源信息的问题。

### 优化层
。。。。TODO：待补充



## 组件技术解析

## 实现架构探索


## 参考资源
* [A Survey of Context Engineering for Large
  Language Models](https://arxiv.org/pdf/2507.13334)
* [Effective context engineering for AI agents](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)
* [提示词工程完全指南](https://www.cnblogs.com/luzhanshi/articles/19071865)
* 

### refs
https://blog.51cto.com/u_16163442/14222622
https://juejin.cn/post/7520818511964749874
https://zhuanlan.zhihu.com/p/1932720788206778299
https://github.com/kaqijiang/context-engineering-intro-zh
https://www.tipkay.com/institute/article/748924819802017792
